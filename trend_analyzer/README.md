# analysis

comparisons made here are w.r.t. experiments from srikant and agrawal's 1996 paper, "mining quantitative association rules in large relational tables".

we are able to reproduce approximately using our model the number of interesting rules for an input with 500,000 rows, 7 attributes (five quantitative and two categorical), min. support of .2, max. support of .4, min. confidence of .25.

|r|k|num. interesting rules (actual)|num. interesting rules (predicted)|
|---|---|---|---|
|1.1|1.5|~2000|2977|
|1.1|2|~400|472|
|1.1|3|~100|236|
|1.1|5|~28|39|
|2|1.5|~200|436|
|2|2|~60|77|
|2|3|~20|39|
|2|5|~4|6|
|1.5|1.5|~1800|2469|
|1.5|2|~200|388|
|1.5|3|~60|190|
|1.5|5|~15|30|

we compare these results with those in figure seven.

we offer a mixture of upper-bounding and average-case-bounding. in this case, we're always upper-bounding and within a factor of four of actual with our upper-/average-bound predictions. some of the misprediction can be attributed to unevenness in the sample data, not extensively taking into account effect of min. confidence, misuse of our fudge factors (utilization fraction range; support-/confidence-related integral approximation; subbotin beta and sigma for utilization spacing, "effective r", p-factor; categorical attribute distinct value count; scale for subbotin for effective r and p-factor; logistic curve shape for interesting item-set count over item-set count ratio), and certain assumptions we make.

assumptions we make include use of whole dense hierarchy and total r liquidity for interesting item-set count over item-set count ratio for phase i and interesting rule count over rule count ratio for phase ii. by "liquidity", we mean an item-set or rule can borrow r from any other item-set or rule, respectively, even if the candidate borrowed-from item-set or rule doesn't have anything that the borrowing item-set or rule does not have. specifically, we might be lacking an entire attribute or a leftover portion of an interval for an attribute that the two item-sets or rules share, which causes upper-bounding. borrowing r means, effectively, we are borrowing support since r = 1 means uniform support density and we are being optimistic with number of r-satisfying nodes by saying as we work our way down from root, borrowing r from below means r for nodes below is decreased from one instead of being unchanged. by "whole", we arbitrarily mean we consider all possible item-sets for a p value for phase i or all possible rules for a p value for phase ii because this means max. degree is two, which simplifies base and exponent for (effective degree) ^ (max. depth given full levels) calculation, but which hurts upper-bounding because degree could be greater than two for the right input rows and we could thereby have r-constraint's restraining effects be lessened; we then say we are average-case-bounding because we converge towards behavior with all possible item-sets or rules as we increase the number of input row collections we see. degree (and r-dodging) can be arbitrarily large if we stray from whole-size hierarchy, but <= n, where n is number of rows, and <= m = 2 ^ (p - 1), where m is number of leaves in whole hierarchy, because this is the max. width for any hierarchy for the item-sets or rules. by "denseness", we mean tightest packing for nodes in the form of a tree, which also causes upper-bounding if used with item-set count upper-bounding for phase i or rule count upper-bounding for phase ii. to not borrow is to assume all nodes satisfy r = 1. there are diminishing returns that cause successive borrowing's lowered effects on increasing effective degree because of inverse correlation between r and effective degree. note that the extreme state associated with indiscriminate (i.e., associated with total liquidity) and optimistic borrowing helps determine a tight packing that satisfies r for phase i and phase ii and helps with upper-bounding. note that this means we are at least three degrees removed from using actual effective degree - we say we can borrow r from any node, we are ignoring the extra cost of handling descendants when we increase r for a node, and degree can be greater than two and thus r can be less of a barrier to increasing interesting item-set over item-set ratio or interesting rule over rule ratio. we do, however, still upper-bound or apply average-case-bounding for some definition of average.

## note on phases

phase i refers to interesting item-set collection determination.

phase ii refers to interesting rule collection determination.

## note on item-sets and rules

r for item-sets says that item-set X has support r times expected support from a close ancestor generalization item-set X\_hat.

r for rules says that rule X has support r times expected support from a close ancestor generalization rule X\_hat.

generalizations and specializations always have same attributes as the current item-set or rule.

a rule X that is r-interesting relative to a rule X\_hat also has its associated same-size (in terms of number of attributes) item-set r-interesting relative to that for X\_hat. however, an item-set that is r-interesting does not have all its associated same-size (in terms of number of attributes) rules necessarily r-interesting. also, a rule that can come from a non-r-interesting item-set can itself be r-interesting because of omission of attributes for rule generation and resulting changing close ancestor relationships; source item-set may have been larger (in terms of number of attributes) than the rule. for example, say item-set {a, b} is not r-interesting because it is not r-interesting relative to close ancestor {a, b''} (which is also not r-interesting) but is r-interesting relative to close ancestor {a, b'}; and {a, b, c} is r-interesting relative to close ancestors {a, b', c} and {a, b'', c} and survives phase i; and a => b can come from {a, b, c} (and not {a, b}) and be r-interesting relative to close ancestor a => b', which came from {a, b'}, and survive phase ii.

rules that have no close ancestors, which happens when we have combinations of attribute values that have no containing rule or we have only attribute values that are maximal in width, automatically pass phase ii.

confidence only applies to rules.

## note on effective r

for "effective r", we use reciprocal of subbotin reverse cdf pass-through because we have truncation effect for low r (can be viewed as saturation and scaling for low r) and we have, to a lesser degree, depression of rules for high r. we have saturation for low r and we have depression for high r beyond that typically for gaussian for the following reasons: max. support causes truncation effect for low r as our abundant amount of small (in terms of number of attributes) rules with large support don't make it past phase ii; and for large r, it's harder for small rules to pass r-interesting because close ancestor support is likely to be high, and so large r may be unattainable because required amount of support is larger than number of input rows even though we use concept of expected support. we note that subbotin is center-heavy and has small tails.

we note that truncation plays a role in phase ii even though we already take into account scaling using max. support by using it to determine number of interesting item-sets because small (in terms of number of attributes) rules can re-surface during phase ii using large (in terms of number of attributes) item-sets.

## details of model

we adjust number of interesting item-sets based on support integral fraction and confidence integral fraction assuming gaussian distribution fixed approximation for r-k combinations. we account for k-completeness' effects on number of item-sets by using left half of subbotin cdf centered at x = 1 so we have behavior in agreement with concave up shape and values less than linear curve for spacing of utilization values. we use utilization values to, based on k and given spacing and min. and max. values, determine effective number of item-sets. specifically, we use the utilization values to modulate base interval counts for quantitative and categorical attributes for effective item-set count. we made an assumption that five options are available for each categorical attribute. we call the effective item-set count "effective n". we use a subbotin reverse cdf centered at x = 1 for "effective r" to modify r. we use a logistic function centered at x = 6 for interesting-item-set over effective n ratio to modulate (2 / r) ^ (log base two of effective n) because 2 / r is effective degree and the power is depth; we have a tree because at worst case, we have all item-set combinations and degree constraint for close ancestors (two for dense hierarchy); and we use logistic function because it behaves more gracefully than log with small (less than one) x. number of interesting item-sets is interesting-item-set over effective n ratio times effective n times adjusted support integral fraction times adjusted confidence integral fraction. number of rules is number of interesting item-sets times 3 ^ p because at worst-case, we have all size-p rules; we can either leave an attribute out, place it at left, or place it at right. for interesting rule count over rule count ratio we use ((2 / effective r) ^ p + max(log base (effective r) of rule count, 0) / (2 ^ p / p-factor). we thought of using effective r because we viewed it as an opportunity to apply subbotin reverse cdf to tame interesting rule count; we are saying that there is a plateau-like distribution for r-interesting value and transforming r into a different r by using a close-to-exponential transform (approaches infinity with increasing x) which is reciprocal of subbotin reverse cdf pass-through. our way of justifying this and being unusually concentrated at r = 1 is that we say it's disproportionately easier to satisfy low r than it is to satisfy high r (this gives us a gaussian) and we have a truncated gaussian because, we hypothesize, max. support leads to truncation effect for small rules, of which we have many, and number of rows becomes more of a constraint for high r, as we note in section "note on effective r"; subbotin is center-heavy and can be viewed as truncated if scaled correctly. effective degree is 2 / effective r and depth is p; we have a tree because at worst case, we have all attribute inclusion combinations (each level is to do with a decision about an attribute from a size-p rule; depth of p is an over-estimate) and degree constraint for close ancestors (two for dense hierarchy for same reason as for phase i). we have max of a log and zero because if we have effective degree of 1, we still have O(depth) internal nodes, and we ignore internal node count in case where number of r-interesting rule leaves per rule predicted is less than one for simplicity. we have 2 ^ p in denominator because this is an upper bound; this is what happens when we have no attenuation due to effective degree. we have p-factor to bring into the picture fact that assuming the average rule is size-p is an over-estimate; to this end, we use a subbotin pdf centered at x = 1 with r. number of interesting rules is interesting rule count over rule count ratio times rule count.

we don't make an extensive attempt to quantify effects of pruning due to min. confidence.

## note on implementation

for item-sets, we sidestep time bottleneck of low min. support with early iterations of candidate generation by using recommended pre-processing step of pruning frequent items with support greater than 1 / r and that have ancestors.

only items that are frequent are used for item-sets.

only item-sets that are r-interesting in terms of support and frequent are used for rules.

only rules that are r-interesting in terms of support and confidence are reported.

## note on close ancestor finding

we break problem of finding close ancestors for all n rectangles into problems of finding close descendants for a particular actual rectangle.

we note that brute force for transitive reduction as **approach #0** takes O(n ^ 3) time.

however, two more difficult approaches can yield better times for a query - O(n \* log(n) ^ 4) for a static solution (**approach #1a**) or O(n \* log(n) ^ 6) for a dynamic solution (**approach #1b**). these times specifically apply to enclosure/containment/intersection/close-descendant queries, and more. we do so by using a bulk-loaded x-tree (for static case) or a bkd-tree with bounding boxes (for dynamic case). we choose either of these structures and have corner transformation, edge testing, and look-ahead. these times are independent of dimension and non-contain to contain overlap ratio (or non-enclose to enclose overlap ratio for containment/close-descendant query or non-intersect to intersect overlap ratio for intersection query). time for priority queue and conflict tree for close-descendant query is hidden by look-ahead time.

see appendix a for details about look-ahead queries for enclosure/containment/intersection/close-descendant queries.

for the r-tree variant approach (**approach #2**), we could make certain assumptions: "maximal disjointedness", "zero-overlap", "uniform sparsity". "maximal disjointedness" refers to state where rectangles at a level are created so as to not overlap with each other as much as possible (given that primitive data may have extent and may unavoidably overlap with each other). "zero-overlap" takes this further, but assumes we have point data, and says that no rectangles at a level overlap with each other. "uniform sparsity" says that as we increase the dimension value, primitive data are spread evenly through the hyper-dimensional space, which implies that items then become, on average, farther away from each other. ignoring all of these assumptions, we can take O(b \* (k + 1) \* log(n)) time for an enclosure/containment/intersection query and O(b \* (k + 1) \* log(n) ^ 2) time for a close-descendant query, where b is false-positive to true-positive ratio (e.g. non-contain to contain overlap ratio for containment query) for a local test of a branch. note that b is in O(n) at worst, but varies with input and can be much better, and better as dimension increases, holding n constant and assuming "uniform sparsity". given that it is more straight-forward to implement an r-tree with queries than our other stated approach, it may be good enough. if we have a [1, U]^d grid (as we do for our quantitative/categorical apriori application) and if we only assume "uniform sparsity", b is on average O((U\_eff / U) ^ d \* n), where U\_eff is number of distinct slots used on average for a dimension. then, if we halve U\_eff, we reduce b by a factor of 2 ^ d. this is one argument in favor of sticking with the r-tree variant approach. for n = 1000, U\_eff = U / 4, d = 1, b is 250. this rather large coefficient, believe it or not, can be fine, especially given the extra effort it would take to improve the algorithm. the running time for all n queries is O(b \* n \* log(n) ^ 2) time because k is on average one for actual rectangles from a rectangle containment hierarchy DAG, and having n queries implies we have a factor of n \* 1 = n. note that it helps to ensure that rectangles are unique to ensure that we have a DAG, at least for our application. also, note that this approach is strictly better than brute force, even for adversarial inputs.

a third choice, **approach #3**, is to view the problem from perspective of getting transitive reduction from closure, but not by using brute force, which traditionally can take up to O(n ^ 3) time. we use enclosure and containment query set union to find middle men, and enclosure query to find whether a path exists or not; these make up tests for path of size >= 2 existence and path of size >= 1 existence. an r-tree variant then would take O(n \* (b \* (k + 1) \* log(n)) \* (b' \* (k + 1) \* log(n))) time, which is un-attractive. r-tree variant gives us inexpensive transitive closure and finding one middle man is all we need to disqualify an ancestor. note that this approach can be strictly worse than approach #2. we can do better.

note that approaches #1a and #1b can be strictly better than approaches #2 and #3.

we note that one example of a rectangle containment hierarchy DAG (which relies on requirement that rectangles are unique) is a linear hierarchy. if at all possible, we want to consider rectangles higher up in the hierarchy earlier for a close descendant query.

for approach #2 (which is inferior in terms of running time relative to approaches #1a and #1b), say we have m dimensions, where m is number of acknowledged attributes. we use a search using a priority queue with the following details: for the purposes of discussing enclosure/containment pruning, we ignore a rectangle subtree if neither the reference rectangle bounds the subtree root rectangle nor the subtree root rectangle bounds the reference rectangle; we ignore the reference rectangle if we come across it; we prefer small containing rectangles and large contained rectangles; we have a best-first priority queue for internal and leaf nodes; we sort rectangles in priority queue by (prefer\_leaf, prefer\_small\_rectangle\_if\_internal\_and\_prefer\_large\_rectangle\_if\_leaf); we on top of ignoring a subtree if reference rectangle is not enclosed/containing ignore if we have a conflict; if we run out of nodes, we stop; we ignore if we have a conflict in the sense that we see if the rectangle is contained by a node for an actual rectangle in a conflict r-tree variant (that we maintain) with early stopping in O(b'' \* log(n)) time. running time for one close descendant query is O(b \* (k + 1) \* log(n) ^ 2), which can be larger than O(n), but is in practice possibly better. close descendant information can be turned, using n queries, into close ancestor information.

for approach #2, for the sake of having vocabulary to describe close-descendant query, we introduce some terms. if we hit a conflict prune, we say we perform a "lower ricochet". lower ricochet takes advantage of conflict r-tree variant by saying that if an internal/leaf node has a containing actual rectangle in the conflict r-tree variant, we're not a close descendant candidate of the current start rectangle. "upper ricochet" is where we temporarily run out of contained rectangles and revert to enclosing rectangles.

we use approach #2 with an x-tree as our r-tree variant.

## appendix a: queries that use look-ahead queries

for approaches #1a and #1b, we can use corner transformation and look-ahead queries for various higher-level queries (e.g. enclosure/containment/intersection/close-descendant). we assume that we have point (2 * d)-tuple (l\_1, r\_1, l\_2, r\_2, ...,  l\_d, r\_d) for an input rectangle. for the purposes of this discussion, we assume we have one dimension, where points are of the form (l, r).

1. enclosure look-ahead - look at bottom and right edges, with query region unbounded at top and at left, recurse using upper-left rectangle within query region
2. containment look-ahead - look at bottom and right edges, with query region unbounded at bottom and at right, recurse using lower-right rectangle within query region
3. intersection look-ahead - look at bottom and right edges, with query region unbounded at top and at left; also, the query region is pushed down by r\_i - l\_i and pushed to the right by r\_i - l\_i, recurse using upper-left rectangle within query region

enclosure, containment, and intersection look-ahead query approaches takes O(log(n)) time for a static approach (using e.g. bulk-loaded x-tree with corner transformation) and O(log(n) ^ 2) time for a dynamic approach (using e.g. bkd-tree with corner transformation). this implies that enclosure, containment, and intersection queries take O(log(n) ^ 2) time for a static approach and O(log(n) ^ 3) time for a dynamic approach. note that these values don't involve d in an exponent (!).

close descendant query can be performed as follows. for one dimension, where intervals are of form [l, r], by x, we mean l value, and by y, we mean r value. store a second-order corner-transformation (i.e. for applying corner transformation twice for (4 * d)-tuple points) conflict tree for each node, which serves to give implicit effect of a pareto front. we use a look-ahead query to guide a close descendant query towards branches in a way that gives, with 100% certainty, that there is a close descendant in a particular branch. insert and delete lead to maintaining of these conflict trees in poly-logarithmic time (not involving d in exponent). for a close descendant query, we "align" to find appropriate "upper-left" leaf (i.e. for max. fan-out equal to two, "upper-left-most" leaf that intersects query point, or if no such node exists, the leaf most to the "upper-left" towards query point) in logarithmic time. (for fan-out greater than two, we arbitrarily deem a node as "upper-left" by choosing among "non-totally-occluded" siblings, i.e. children with bounding boxes that have "lower-left" and "upper-right" points that are not totally dominated by such points for bounding boxes for their other siblings.) we move from this node to highest node we care about (i.e., the query node, and sometimes not root) by visiting ancestors. for each node and for each dimension, we keep the value of that component for a point in the subtree s.t. it is closest to "upper-left" (i.e., for one dimension and max. fan-out of two, for "bottom" store min. x and for "right" store max. y). if we are at leaf "upper-left", we can perform containment query using query point to see if we have a non-dominated point (i.e., a close descendant) with early stopping at this node. if we are "upper-left", and our sibling (assuming fan-out is two and not more, though we can generalize as needed) is "bottom", use "left-most" x from "upper-left" node together with query x to perform a three-sided query (for one dimension, though we can generalize by allowing any values for other dimensions) in "quadrant III" to get contributing pareto set non-emptiness for "bottom" with early stopping. if we are "upper-left", and our sibling is "right", use "top-most" y from "upper-left" node together with query y to perform a three-sided query in "quadrant I" to get contributing pareto set non-emptiness for "right" with early stopping. if we are "bottom", and our sibling is "upper-left", ignore "upper-left". if we are "right", and our sibling is "upper-left", ignore "upper-left". to perform three-sided queries with d > 1, we perform a range query with, for a particular dimension, a component for one corner set to +- infinity (specifically, one corner's y for "quadrant III" set to -infinity or one corner's x for "quadrant I" set to +infinity) and for the remaining dimensions unboundedness. also, for the three-sided queries, we use a node-specific second-order corner-transformation conflict tree. (if fan-out is greater than two, only work with "non-totally-occluded" children and, for each dimension, perform a pareto set non-emptiness test if the child's rectangle juts out in this dimension toward either "left" or "upper" directions w.r.t. chosen "upper-left" child, given that bounding boxes are disjoint; note that a lack of jutting out for a dimension means we can ignore the rectangle for this dimension. for non-emptiness test and x-jutting rectangle, we use, for a dimension d\_i, for x axis, an always-existing next largest x using a scan over O(fan-out) rectangles can be found to determine, after checking that we jut out relative to chosen "upper-left" rectangle, the d\_i-centric "quadrant III" non-emptiness test x upper-bound (instead of "upper-left" node "left-most" x). for this, we also allow unboundedness for dimensions that are not d\_i. for non-emptiness test and y-jutting rectangle, we use, for a dimension d\_i, for y axis, an always-existing next smallest y using a scan over O(fan-out) rectangles can be found to determine, after checking that we jut out relative to chosen "upper-left" rectangle, the d\_i-centric "quadrant I" non-emptiness test y lower-bound (instead of "upper-left" node "top-most" y). for this, we also allow unboundedness for dimensions that are not d\_i.) the time for maintenance of trees and thus time for insert/delete is pushed up (temporarily; see an addendum) to O(log(n) ^ 2) (static) or O(log(n) ^ 3) (dynamic). our close descendant look-ahead query takes O(log(n)) time to "align", involves O(log(n)) levels per invocation, takes for level-specific range queries O(log(n) ^ 2) time (static) or O(log(n) ^ 3) time (dynamic). close descendant look-ahead then takes, overall, O(log(n) ^ 3) time (static) or O(log(n) ^ 4) time (dynamic). close descendant query then takes O(k * log(n) ^ 4) time (static) or O(k * log(n) ^ 6) time (dynamic, assuming bkd-tree). note that these values don't involve d in an exponent (!).

as it turns out, insert and delete don't have to be slower for if we wish to support close descendant query. we modify "three-sided" (named so because we modulate a quadrant-III or quadrant-I query by restricting x or y for one dimension, and have unmodulated quadrant-III or quadrant-I query for rest of the dimensions) queries that we perform using second-order containment queries by changing the associated first-order regions to be cropped by the bounding box in all 2 * d dimensions of the root of the subtree of interest. this way, we don't need to store a second-order corner-transformation conflict tree for every node; we only need to store one for the root. note that a first-order (2 * d)-tuple point is also a second-order (4 * d)-tuple point. the argument is that by cropping the first-order query range, we are tailoring the search for a subtree. for if max. fan-out is two, if a point lies on a "left" boundary (as "bottom") or "upper" boundary (as "right"), then there is some ambiguity as to whether that point is in the subtree that has the "bottom" or "right" node as a child (i.e. it could be in a different subtree or it could be in the "bottom" or "right" node). but, if this happens, then we just chose the wrong "upper-left" leaf for "alignment". if we don't choose the wrong "upper-left" leaf for alignment, i.e., there are not regions both more to the "left" and to the "upper" directions that are either intersected by the query point (if there indeed exists a leaf that is intersected by the query point) or there are not regions simultaneously more to the "left" and to the "upper" directions that are in the query region, then this ambiguity never happens. note that we are dealing with bounding boxes, and not regions (which don't exist for bulk-loaded x-tree). also, as discussed earlier, fan-out can be greater than two; in this case, it is safe for us to arbitrarily choose an "upper-left" node among ties (any leaf that is "non-totally-occluded" w.r.t. siblings). for if max. fan-out is two, also, we may have a point on a boundary between "upper-left" and "bottom" or between "upper-left" and "right"; this "friendly blurred border" ambiguity is not an issue, however, because they are all part of same sub-tree (the node that has "upper-left" and "bottom" or "upper-left" and "right" as children), the root of which is what we are assessing. (if max. fan-out is greater than two, the first ambiguity is not an issue if we choose "upper-left" leaf correctly and the second ambiguity also arises benignly between child rectangles.) this way, insert and delete for when we have close descendant query also take O(log(n)) time (for static case) or O(log(n) ^ 2) time (for dynamic case) (!). this approach generalizes to when we have d > 1 and the times don't involve d in an exponent (!).

## references

1. agrawal and srikant: fast algorithms for mining association rules (1994)
2. berchtold et al.: the x-tree: an index structure for high-dimensional data (1996)
3. berchtold et al.: improving the query performance of high-dimensional index structures by bulk load operations (1998)
4. goebel: towards logarithmic search time complexity for r-trees (2007)
5. pagel et. al: the transformation technique for spatial objects revisited (1993)
6. procopiuc et al.: bkd-tree: a dynamic scalable kd-tree (2003)
7. srikant and agrawal: mining quantitative association rules in large relational tables (1996)


